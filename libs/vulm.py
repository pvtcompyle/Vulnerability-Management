#import os
#import glob
#import datetime
#import re
#import pandas as pd



def getcsv():
    import glob
    extension = 'csv'
    files = glob.glob('*.{}'.format(extension))
    return(files)

def consolidatescans():
    import re
    global scantype
    for f in files:
        print('Processing: ', f)
        scantype = re.search('Dynamic_(.*?)_Scan', f).group(1)
        data = pd.read_csv(f, sep=',')
        if f==files[0]:
            data.insert(0, "ScanType", scantype, allow_duplicates=True)
            data.insert(1, "ScanDate", scandate, allow_duplicates=True)
            data.to_csv(logfile, sep=',', index=False)
        else:
            data.insert(0, "ScanType", scantype, allow_duplicates=True)
            data.insert(1, "ScanDate", scandate, allow_duplicates=True)        
            data.to_csv(logfile, mode='a', sep=',', header=False, index=False)
    print('Consolidation Complete')

def parsetrash(pfile):
    import pandas as pd
    print('Parsing results to exclude Low and Informational vulnerabilities...')
    data = pd.read_csv(pfile, sep=',')
    data = data[data['CVSS'] > 3.9]
    data = data.sort_values(['Host', 'CVSS'], ascending=[True, False])
    data.to_csv(parsefile, sep=',', index=False)
    print('Trash results below Medium are parsed.')

def seperatehosts():
    global parsefile
    hosts_frame = pd.read_csv(parsefile)
    hosts = hosts_frame['Host'].unique()
    
    for h in hosts:
        result = hosts_frame[hosts_frame['Host'].str.match(h)]
        hostfile = 'results\\hosts\\' + h + '.csv'
        hostfile = os.path.join(dir, hostfile)
        print('Processing ', hostfile)
        result.to_csv(hostfile, sep=',', index=False)
    print('Hosts have been seperated into individual files.')